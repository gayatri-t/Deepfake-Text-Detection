{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94b1b8db-3477-43c4-829d-1aecc145e2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading Roberta tokenizer...\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/vprathi/.local/lib/python3.9/site-packages/transformers/training_args.py:1454: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n",
      "eval_loss: -0.03537864238023758\n",
      "eval_accuracy: 0.4935\n",
      "eval_precision: 0.4966512107161257\n",
      "eval_recall: 0.964\n",
      "eval_f1: 0.6555593335600136\n",
      "eval_auc: 0.35198999999999997\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import jsonlines\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import logging\n",
    "\n",
    "def dummy_data_collector(features):\n",
    "    batch = {}\n",
    "    batch['input_ids'] = torch.stack([f[0] for f in features])\n",
    "    batch['attention_mask'] = torch.stack([f[1] for f in features])\n",
    "    batch['labels'] = torch.stack([f[2] for f in features])\n",
    "    return batch\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    acc = np.mean(preds == p.label_ids.reshape(-1))\n",
    "    \n",
    "    precision, recall, fscore, _ = score(p.label_ids.reshape(-1), preds, average='binary')\n",
    "    \n",
    "    # Compute AUC\n",
    "    if p.label_ids.shape[1] > 1:  # Check for multi-class classification\n",
    "        auc_test = roc_auc_score(p.label_ids, p.predictions, multi_class='ovr')\n",
    "    else:\n",
    "        auc_test = roc_auc_score(p.label_ids, p.predictions[:, 1])\n",
    "    \n",
    "    return {\n",
    "        \"eval_loss\": p.predictions.mean().item(),\n",
    "        \"eval_accuracy\": acc,\n",
    "        \"eval_precision\": precision,\n",
    "        \"eval_recall\": recall,\n",
    "        \"eval_f1\": fscore,\n",
    "        \"eval_auc\": auc_test,\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO, format='%(message)s')  # Modified logging format\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    logger.info('Loading Roberta tokenizer...')\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "    labels = []\n",
    "    all_articles = []\n",
    "    with jsonlines.open('GPT.jsonl', 'r') as input_articles:\n",
    "        for article in input_articles:\n",
    "            all_articles.append(article['text'])\n",
    "            labels.append(article['label'])\n",
    "\n",
    "    encoded_article = tokenizer.batch_encode_plus(\n",
    "        all_articles,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        padding='longest',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_article['input_ids']\n",
    "    attention_masks = encoded_article['attention_mask']\n",
    "    \n",
    "    labels = np.asarray(labels)\n",
    "    labels = np.expand_dims(np.where((labels == 'machine'), 1, 0), 1)\n",
    "    labels = torch.from_numpy(labels)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        'roberta-base',\n",
    "        num_labels=2,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        logging_dir='./logs',\n",
    "        evaluation_strategy=\"steps\",\n",
    "        do_predict=True,\n",
    "        eval_steps=500,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=dummy_data_collector,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    predictions = trainer.predict(dataset)\n",
    "    \n",
    "    # Extracting necessary values from PredictionOutput object\n",
    "    loss = np.mean(predictions.predictions)\n",
    "    acc = np.mean(np.argmax(predictions.predictions, axis=1) == predictions.label_ids.reshape(-1))\n",
    "    \n",
    "    precision, recall, fscore, _ = score(predictions.label_ids.reshape(-1), \n",
    "                                         np.argmax(predictions.predictions, axis=1), \n",
    "                                         average='binary')\n",
    "    \n",
    "    if predictions.label_ids.shape[1] > 1:\n",
    "        auc_test = roc_auc_score(predictions.label_ids, predictions.predictions, multi_class='ovr')\n",
    "    else:\n",
    "        auc_test = roc_auc_score(predictions.label_ids, predictions.predictions[:, 1])\n",
    "    \n",
    "    eval_results = {\n",
    "        \"eval_loss\": loss,\n",
    "        \"eval_accuracy\": acc,\n",
    "        \"eval_precision\": precision,\n",
    "        \"eval_recall\": recall,\n",
    "        \"eval_f1\": fscore,\n",
    "        \"eval_auc\": auc_test,\n",
    "    }\n",
    "    \n",
    "    # Print the results in the desired format\n",
    "    print(\"Evaluation results:\")\n",
    "    for key, value in eval_results.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e21ac85-2f3c-4d89-9d92-da325bf1653a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Roberta tokenizer...\n",
      "Unique labels: [0 1], Counts: [1000 1000]\n",
      "/home/vprathi/.local/lib/python3.9/site-packages/transformers/optimization.py:520: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/vprathi/.local/lib/python3.9/site-packages/transformers/training_args.py:1454: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 05:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.687700</td>\n",
       "      <td>0.945108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample predictions: [[-0.48676005  0.09830824]\n",
      " [-0.48674285  0.09826421]\n",
      " [-0.48674405  0.09827331]\n",
      " [-0.4867698   0.09832444]\n",
      " [-0.48673174  0.098283  ]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 04:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 evaluation results:\n",
      "eval_loss: 1.027869462966919\n",
      "eval_accuracy: 0.0\n",
      "eval_precision: 0.0\n",
      "eval_recall: 0.0\n",
      "eval_f1: 0.0\n",
      "eval_auc: 0.0\n",
      "eval_runtime: 8.0449\n",
      "eval_samples_per_second: 49.721\n",
      "eval_steps_per_second: 6.215\n",
      "epoch: 3.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 05:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.681000</td>\n",
       "      <td>0.940912</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample predictions: [[-0.4073565  -0.0441561 ]\n",
      " [-0.40677866 -0.04466706]\n",
      " [-0.40595853 -0.045229  ]\n",
      " [-0.40803176 -0.04358913]\n",
      " [-0.40634042 -0.04496273]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 04:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting epoch 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 evaluation results:\n",
      "eval_loss: 0.8909348845481873\n",
      "eval_accuracy: 0.0\n",
      "eval_precision: 0.0\n",
      "eval_recall: 0.0\n",
      "eval_f1: 0.0\n",
      "eval_auc: 0.0\n",
      "eval_runtime: 8.0625\n",
      "eval_samples_per_second: 49.612\n",
      "eval_steps_per_second: 6.202\n",
      "epoch: 3.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 05:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample predictions: [[ 5.071643  -4.7022247]\n",
      " [ 5.071638  -4.702224 ]\n",
      " [ 5.071645  -4.7022233]\n",
      " [ 5.0716424 -4.7022243]\n",
      " [ 5.0716443 -4.702227 ]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 evaluation results:\n",
      "eval_loss: 5.6861204939195886e-05\n",
      "eval_accuracy: 1.0\n",
      "eval_precision: 0.0\n",
      "eval_recall: 0.0\n",
      "eval_f1: 0.0\n",
      "eval_auc: 0.0\n",
      "eval_runtime: 8.0546\n",
      "eval_samples_per_second: 49.661\n",
      "eval_steps_per_second: 6.208\n",
      "epoch: 3.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import jsonlines\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, Trainer, TrainingArguments, RobertaConfig, AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import logging\n",
    "\n",
    "def dummy_data_collector(features):\n",
    "    batch = {}\n",
    "    batch['input_ids'] = torch.stack([f[0] for f in features])\n",
    "    batch['attention_mask'] = torch.stack([f[1] for f in features])\n",
    "    batch['labels'] = torch.stack([f[2] for f in features])\n",
    "    return batch\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    acc = np.mean(preds == p.label_ids.reshape(-1))\n",
    "    \n",
    "    precision, recall, fscore, _ = score(p.label_ids.reshape(-1), preds, average='binary', zero_division=0)\n",
    "    \n",
    "    # Check if both classes are present in labels\n",
    "    unique_labels = np.unique(p.label_ids)\n",
    "    if len(unique_labels) == 2:\n",
    "        if p.label_ids.shape[1] > 1:  # Check for multi-class classification\n",
    "            auc_test = roc_auc_score(p.label_ids, p.predictions, multi_class='ovr')\n",
    "        else:\n",
    "            auc_test = roc_auc_score(p.label_ids, p.predictions[:, 1])\n",
    "    else:\n",
    "        auc_test = 0.0  # Set AUC to 0 if only one class is present\n",
    "    \n",
    "    return {\n",
    "        \"eval_loss\": p.predictions.mean().item(),\n",
    "        \"eval_accuracy\": acc,\n",
    "        \"eval_precision\": precision,\n",
    "        \"eval_recall\": recall,\n",
    "        \"eval_f1\": fscore,\n",
    "        \"eval_auc\": auc_test,\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    logger.info('Loading Roberta tokenizer...')\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "    labels = []\n",
    "    all_articles = []\n",
    "    with jsonlines.open('GPT.jsonl', 'r') as input_articles:\n",
    "        for article in input_articles:\n",
    "            all_articles.append(article['text'])\n",
    "            labels.append(article['label'])\n",
    "\n",
    "    encoded_article = tokenizer.batch_encode_plus(\n",
    "        all_articles,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        padding='longest',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_article['input_ids']\n",
    "    attention_masks = encoded_article['attention_mask']\n",
    "    \n",
    "    labels = np.asarray(labels)\n",
    "    labels = np.expand_dims(np.where((labels == 'machine'), 1, 0), 1)\n",
    "    labels = torch.from_numpy(labels)\n",
    "\n",
    "    # Print unique labels and counts\n",
    "    unique_labels, counts = np.unique(labels.numpy(), return_counts=True)\n",
    "    logger.info(f\"Unique labels: {unique_labels}, Counts: {counts}\")\n",
    "\n",
    "    # Splitting dataset into training and evaluation sets\n",
    "    train_size = int(0.8 * len(input_ids))\n",
    "    eval_size = len(input_ids) - train_size\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(input_ids[:train_size], \n",
    "                                                   attention_masks[:train_size], \n",
    "                                                   labels[:train_size])\n",
    "\n",
    "    eval_dataset = torch.utils.data.TensorDataset(input_ids[train_size:], \n",
    "                                                  attention_masks[train_size:], \n",
    "                                                  labels[train_size:])\n",
    "\n",
    "    config = RobertaConfig.from_pretrained('roberta-base', num_labels=2)\n",
    "    model = RobertaForSequenceClassification(config)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps=0,\n",
    "                                                num_training_steps=len(train_dataset) * 3)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        logging_dir='./logs',\n",
    "        evaluation_strategy=\"steps\",\n",
    "        do_predict=True,\n",
    "        eval_steps=500,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=dummy_data_collector,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    for epoch in range(training_args.num_train_epochs):\n",
    "        logger.info(f\"Starting epoch {epoch + 1}\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Debugging: Print some predictions during training\n",
    "        predictions = trainer.predict(eval_dataset)\n",
    "        logger.info(f\"Sample predictions: {predictions.predictions[:5]}\")\n",
    "        \n",
    "        eval_results = trainer.evaluate()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1} evaluation results:\")\n",
    "        for key, value in eval_results.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4021315-5173-4e68-a047-70c990f58f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
