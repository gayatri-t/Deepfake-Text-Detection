{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bfea761-8958-4616-8882-e4695bd75290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-12GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vprathi/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5235\n",
      "auc_test: 0.6215189999999999\n",
      "precision_human: 0.5137507314218841\n",
      "recall_human: 0.878\n",
      "fscore_human: 0.6482096714654855\n",
      "support_human: 1000.0\n",
      "precision_machine: 0.5807560137457045\n",
      "recall_machine: 0.169\n",
      "fscore_machine: 0.2618125484120836\n",
      "support_machine: 1000.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import jsonlines\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import tempfile\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:1\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def dummy_data_collector(features):\n",
    "    batch = {}\n",
    "    batch['input_ids'] = torch.stack([f[0] for f in features])\n",
    "    batch['attention_mask'] = torch.stack([f[1] for f in features])\n",
    "    batch['labels'] = torch.stack([f[2] for f in features])\n",
    "    return batch\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    precision, recall, fscore, support = score(p.label_ids.reshape(-1), preds)\n",
    "\n",
    "    auc_test = roc_auc_score(p.label_ids.reshape(-1), p.predictions[:, 1])\n",
    "    return {\n",
    "        \"acc\": np.mean(preds == p.label_ids.reshape(-1)),\n",
    "        \"auc_test\": auc_test,\n",
    "        \"precision_human\": precision[0],\n",
    "        \"recall_human\": recall[0],\n",
    "        \"fscore_human\": fscore[0],\n",
    "        \"support_human\": float(support[0]),\n",
    "        \"precision_machine\": precision[1],\n",
    "        \"recall_machine\": recall[1],\n",
    "        \"fscore_machine\": fscore[1],\n",
    "        \"support_machine\": float(support[1])\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    prediction_output = \"./EVAL_processed_webtext_eval_tokens_topp_096_4k.jsonl\"\n",
    "    test_dir = \"./GPT.jsonl\"\n",
    "    train_batch_size = 2\n",
    "    val_batch_size = 32\n",
    "    num_train_epochs = 1\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "\n",
    "    labels = []\n",
    "    all_articles_test = []\n",
    "\n",
    "    with jsonlines.open(test_dir, 'r') as input_articles:\n",
    "        for article in input_articles:\n",
    "            all_articles_test.append(article['text'])\n",
    "            labels.append(article['label'])\n",
    "\n",
    "    labels = np.asarray(labels)\n",
    "    labels = np.expand_dims(np.where((labels == 'machine'), 1, 0), 1)\n",
    "    labels_test = torch.from_numpy(labels)\n",
    "\n",
    "    input_ids_test = []\n",
    "    attention_masks_test = []\n",
    "\n",
    "    for article in all_articles_test:\n",
    "        encoded_dict = tokenizer(article, return_tensors=\"pt\", pad_to_max_length=True, truncation=True, max_length=512)\n",
    "        input_ids_test.append(encoded_dict['input_ids'])\n",
    "        attention_masks_test.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    input_ids_test = torch.cat(input_ids_test, dim=0)\n",
    "    attention_masks_test = torch.cat(attention_masks_test, dim=0)\n",
    "\n",
    "    test_dataset = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-large-cased',\n",
    "        num_labels=2,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False\n",
    "    )\n",
    "\n",
    "   # Create a temporary directory for output_dir\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=tmp_dir,  # Use temporary directory\n",
    "            overwrite_output_dir=True,\n",
    "            do_predict=True,\n",
    "            per_device_train_batch_size=train_batch_size,\n",
    "            per_device_eval_batch_size=val_batch_size,\n",
    "            num_train_epochs=num_train_epochs\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            data_collator=dummy_data_collector,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "        preds_man = trainer.predict(test_dataset)\n",
    "    preds_man = trainer.predict(test_dataset)\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    metrics = compute_metrics(preds_man)\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74874c6-4649-4f07-83a3-dab37c123155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02446bd0-1bb6-460a-9ec5-222d13f13c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-12GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [175/175 07:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Fscore</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.721400</td>\n",
       "      <td>0.542839</td>\n",
       "      <td>0.659898</td>\n",
       "      <td>0.921986</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.500600</td>\n",
       "      <td>0.469555</td>\n",
       "      <td>0.702564</td>\n",
       "      <td>0.971631</td>\n",
       "      <td>0.815476</td>\n",
       "      <td>0.793333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.419100</td>\n",
       "      <td>0.417316</td>\n",
       "      <td>0.704082</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.818991</td>\n",
       "      <td>0.796667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.346600</td>\n",
       "      <td>0.459609</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.780142</td>\n",
       "      <td>0.805861</td>\n",
       "      <td>0.823333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.373600</td>\n",
       "      <td>0.721681</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.198582</td>\n",
       "      <td>0.331361</td>\n",
       "      <td>0.623333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.546300</td>\n",
       "      <td>0.436562</td>\n",
       "      <td>0.851562</td>\n",
       "      <td>0.773050</td>\n",
       "      <td>0.810409</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.402600</td>\n",
       "      <td>0.359715</td>\n",
       "      <td>0.758427</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.846395</td>\n",
       "      <td>0.836667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.308800</td>\n",
       "      <td>0.342691</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.832117</td>\n",
       "      <td>0.846667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_precision: 0.9173\n",
      "eval_recall: 0.7974\n",
      "eval_fscore: 0.8531\n",
      "eval_accuracy: 0.8600\n",
      "eval_loss: 0.3137\n",
      "eval_runtime: 21.6850\n",
      "eval_samples_per_second: 13.8340\n",
      "eval_steps_per_second: 3.4590\n",
      "epoch: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import jsonlines\n",
    "import random\n",
    "import numpy as np\n",
    "import logging\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments, EvalPrediction\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Check for GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.device_count() > 1 else \"cuda:0\")\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Data collector for dummy batching\n",
    "def dummy_data_collector(features):\n",
    "    batch = {}\n",
    "    batch['input_ids'] = torch.stack([f[0] for f in features])\n",
    "    batch['attention_mask'] = torch.stack([f[1] for f in features])\n",
    "    batch['labels'] = torch.stack([f[2] for f in features])\n",
    "    return batch\n",
    "\n",
    "# Compute metrics for evaluation\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(\n",
    "        p.label_ids, preds, average='binary'\n",
    "    )\n",
    "    accuracy = np.mean(preds == p.label_ids)\n",
    "    \n",
    "    return {\n",
    "        \"eval_precision\": precision,\n",
    "        \"eval_recall\": recall,\n",
    "        \"eval_fscore\": fscore,\n",
    "        \"eval_accuracy\": accuracy,\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "\n",
    "    # Read gpt.jsonl file\n",
    "    all_articles = []\n",
    "    with jsonlines.open('GPT.jsonl', 'r') as input_articles:\n",
    "        for article in input_articles:\n",
    "            all_articles.append(article)\n",
    "    \n",
    "    # Shuffle and split data\n",
    "    random.shuffle(all_articles)\n",
    "    train_articles = all_articles[:int(0.7 * len(all_articles))]\n",
    "    val_articles = all_articles[int(0.7 * len(all_articles)):int(0.85 * len(all_articles))]\n",
    "    test_articles = all_articles[int(0.85 * len(all_articles)):]\n",
    "\n",
    "    # Preprocess data\n",
    "    def preprocess(articles):\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        labels = []\n",
    "        for article in articles:\n",
    "            encoded_dict = tokenizer(article['text'], return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512)\n",
    "            input_ids.append(encoded_dict['input_ids'].squeeze())\n",
    "            attention_masks.append(encoded_dict['attention_mask'].squeeze())\n",
    "            labels.append(1 if article['label'] == 'machine' else 0)\n",
    "        return torch.stack(input_ids), torch.stack(attention_masks), torch.tensor(labels)\n",
    "    \n",
    "    train_input_ids, train_attention_masks, train_labels = preprocess(train_articles)\n",
    "    val_input_ids, val_attention_masks, val_labels = preprocess(val_articles)\n",
    "    test_input_ids, test_attention_masks, test_labels = preprocess(test_articles)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "    val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "    test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "\n",
    "    # Load model\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-large-cased', num_labels=2)\n",
    "    model.to(device)\n",
    "\n",
    "      # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=1,                # Reduced to 2 epochs\n",
    "        per_device_train_batch_size=4,  \n",
    "        per_device_eval_batch_size=4,   \n",
    "        gradient_accumulation_steps=2,  \n",
    "        save_steps=100,                   # Reduced save steps\n",
    "        evaluation_strategy=\"steps\",\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=20,                  # Reduced logging steps\n",
    "        fp16=True,                       \n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=dummy_data_collector\n",
    "    )\n",
    "\n",
    "    # Train and evaluate\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    for key, value in eval_results.items():\n",
    "        print(f\"{key}: {value:.4f}\" if isinstance(value, float) else f\"{key}: {value}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3d0807-0930-43a6-90d5-1eb1410d8b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc847b2f-7877-44f0-8df8-82de476854e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-12GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/vprathi/.local/lib/python3.9/site-packages/transformers/training_args.py:1454: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [350/350 11:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Fscore</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.773300</td>\n",
       "      <td>0.698260</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.651685</td>\n",
       "      <td>0.483333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.735700</td>\n",
       "      <td>0.693330</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.651685</td>\n",
       "      <td>0.483333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.713700</td>\n",
       "      <td>0.694025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.715900</td>\n",
       "      <td>0.696108</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.651685</td>\n",
       "      <td>0.483333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.696900</td>\n",
       "      <td>0.709687</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.651685</td>\n",
       "      <td>0.483333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.730100</td>\n",
       "      <td>0.693511</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.651685</td>\n",
       "      <td>0.483333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.664200</td>\n",
       "      <td>0.697026</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.651685</td>\n",
       "      <td>0.483333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.738900</td>\n",
       "      <td>0.692604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.691100</td>\n",
       "      <td>0.696704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.691100</td>\n",
       "      <td>0.699725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.746100</td>\n",
       "      <td>0.692563</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>0.700109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.715800</td>\n",
       "      <td>0.700700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.701300</td>\n",
       "      <td>0.692588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.712900</td>\n",
       "      <td>0.696154</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.651685</td>\n",
       "      <td>0.483333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.689300</td>\n",
       "      <td>0.696624</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.651685</td>\n",
       "      <td>0.483333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.714800</td>\n",
       "      <td>0.700939</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.651685</td>\n",
       "      <td>0.483333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_precision: 0.5100\n",
      "eval_recall: 1.0000\n",
      "eval_fscore: 0.6755\n",
      "eval_accuracy: 0.5100\n",
      "eval_loss: 0.6960\n",
      "eval_runtime: 21.6529\n",
      "eval_samples_per_second: 13.8550\n",
      "eval_steps_per_second: 6.9270\n",
      "epoch: 1.0000\n",
      "Evaluation with Beam Search:\n",
      "eval_precision: 0.0000\n",
      "eval_recall: 0.0000\n",
      "eval_fscore: 0.0000\n",
      "eval_accuracy: 0.4900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import jsonlines\n",
    "import random\n",
    "import numpy as np\n",
    "import logging\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments, EvalPrediction\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Check for GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.device_count() > 1 else \"cuda:0\")\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Data collector for dummy batching\n",
    "def dummy_data_collector(features):\n",
    "    batch = {}\n",
    "    batch['input_ids'] = torch.stack([f[0] for f in features])\n",
    "    batch['attention_mask'] = torch.stack([f[1] for f in features])\n",
    "    batch['labels'] = torch.stack([f[2] for f in features])\n",
    "    return batch\n",
    "\n",
    "# Compute metrics for evaluation\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(\n",
    "        p.label_ids, preds, average='binary'\n",
    "    )\n",
    "    accuracy = np.mean(preds == p.label_ids)\n",
    "    \n",
    "    return {\n",
    "        \"eval_precision\": precision,\n",
    "        \"eval_recall\": recall,\n",
    "        \"eval_fscore\": fscore,\n",
    "        \"eval_accuracy\": accuracy,\n",
    "    }\n",
    "\n",
    "def beam_search(logits, beam_size=3):\n",
    "    num_labels = logits.size(1)\n",
    "    if beam_size > num_labels:\n",
    "        beam_size = num_labels\n",
    "    \n",
    "    # Flatten the logits to have shape [batch_size * seq_len, num_labels]\n",
    "    logits = logits.view(-1, logits.size(-1))\n",
    "\n",
    "    topk = torch.topk(logits, beam_size, dim=1)\n",
    "    topk_indices = topk.indices\n",
    "    topk_scores = topk.values\n",
    "\n",
    "    beams = [([], 1.0)]  # List of (sequence, score)\n",
    "\n",
    "    for i in range(logits.size(0)):\n",
    "        new_beams = []\n",
    "        for beam_seq, beam_score in beams:\n",
    "            for next_label in topk_indices[i]:\n",
    "                new_seq = beam_seq + [next_label.item()]\n",
    "                new_score = beam_score * topk_scores[i][next_label].item()\n",
    "                new_beams.append((new_seq, new_score))\n",
    "        \n",
    "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "    \n",
    "    return max(beams, key=lambda x: x[1])[0]\n",
    "\n",
    "\n",
    "def evaluate_with_beam_search(model, eval_dataset, beam_size=3):\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for input_ids, attention_mask, labels in DataLoader(eval_dataset, batch_size=4):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids.to(device), attention_mask=attention_mask.to(device))[0]\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            preds = beam_search(probs, beam_size)\n",
    "            \n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.tolist())\n",
    "    \n",
    "    return all_preds, all_labels\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "\n",
    "    # Read GPT.jsonl file\n",
    "    all_articles = []\n",
    "    with jsonlines.open('GPT.jsonl', 'r') as input_articles:\n",
    "        for article in input_articles:\n",
    "            all_articles.append(article)\n",
    "    \n",
    "    # Shuffle and split data\n",
    "    random.shuffle(all_articles)\n",
    "    train_articles = all_articles[:int(0.7 * len(all_articles))]\n",
    "    val_articles = all_articles[int(0.7 * len(all_articles)):int(0.85 * len(all_articles))]\n",
    "    test_articles = all_articles[int(0.85 * len(all_articles)):]\n",
    "\n",
    "    # Preprocess data\n",
    "    def preprocess(articles):\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        labels = []\n",
    "        for article in articles:\n",
    "            encoded_dict = tokenizer(article['text'], return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512)\n",
    "            input_ids.append(encoded_dict['input_ids'].squeeze())\n",
    "            attention_masks.append(encoded_dict['attention_mask'].squeeze())\n",
    "            labels.append(1 if article['label'] == 'machine' else 0)\n",
    "        return torch.stack(input_ids), torch.stack(attention_masks), torch.tensor(labels)\n",
    "    \n",
    "    train_input_ids, train_attention_masks, train_labels = preprocess(train_articles)\n",
    "    val_input_ids, val_attention_masks, val_labels = preprocess(val_articles)\n",
    "    test_input_ids, test_attention_masks, test_labels = preprocess(test_articles)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "    val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "    test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "\n",
    "    # Load model\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-large-cased', num_labels=2)\n",
    "    model.to(device)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        save_steps=100,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=20,\n",
    "        fp16=True,\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=dummy_data_collector\n",
    "    )\n",
    "\n",
    "    # Train and evaluate\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    for key, value in eval_results.items():\n",
    "        print(f\"{key}: {value:.4f}\" if isinstance(value, float) else f\"{key}: {value}\")\n",
    "\n",
    "    # Evaluate with beam search\n",
    "    all_preds, all_labels = evaluate_with_beam_search(model, test_dataset)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "    accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "    \n",
    "    # Print evaluation metrics with beam search\n",
    "    print(\"Evaluation with Beam Search:\")\n",
    "    print(f\"eval_precision: {precision:.4f}\")\n",
    "    print(f\"eval_recall: {recall:.4f}\")\n",
    "    print(f\"eval_fscore: {fscore:.4f}\")\n",
    "    print(f\"eval_accuracy: {accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9263b6a9-fb85-4aae-9552-dfeb58f1d0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52071b47-40df-499c-8861-27e518bbb6a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
